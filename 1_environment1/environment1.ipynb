{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef18a1ab",
   "metadata": {},
   "source": [
    "## The advantage of using eligibility traces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c76839",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Imports and examples](#Imports-And-Examples)\n",
    "3. [One step horizon](#One-step-horizon)\n",
    "4. [Implementation of TD-algorithms](#your-implementations)\n",
    "5. [Test your algorithms](#test-your-algos)\n",
    "6. [Exploration-Exploitation dilemma](#Exploration-vs-Exploration)\n",
    "7. [Eligibility traces](#traces)\n",
    "8. [Bonus questions](#bonus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee60e9",
   "metadata": {},
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509e624",
   "metadata": {},
   "source": [
    "In this first computational exercise session, you will learn how eligibility traces can lead to more efficient training. As you have discussed in the lecture, standard temporal-difference methods, such as Q-Learning or Sarsa, leverage bootstrapping and update $Q$-values based on the consistency relation derived by the Bellman equation, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for $Q$-Learning and\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    Q(s_t, a_t) = Q(s_t, a_t) + \\alpha (r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "for Sarsa. These kind of updates do not take into account the history of training before time $t$ and therefore, their performance slows down for refined discretization schemes of a given environment.\n",
    "\n",
    "Eligibility traces are one of the basic mechanisms of reinforcement learning. They are implemented by defining a shadow variable $e(s, a)$ for each state-action pair $(s, a)$ and they can be combined with almost any temporal-difference (TD) method, such as Q-learning or Sarsa, to obtain a more general method that learns more quickly and more efficiently. When TD methods are augmented with eligibility traces, they produce a family of methods spanning a spectrum that has Monte Carlo methods at one end and one-step TD methods at the other.\n",
    "\n",
    "An eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. This helps propagating the information back from the rewarded states to the initial states in a faster and more robust way.\n",
    "\n",
    "For the sake of convenience, you find below the pseudocode for Sarsa($\\lambda$) with eligibility traces.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{For}  \\hspace{2pt} &i = 1, \\cdots, n\\\\\n",
    "&\\text{Set} \\hspace{2pt} e(s, a) = 0  \\hspace{2pt} \\forall (s, a) \\\\\n",
    "&\\text{While current episode is not ended} \\\\\n",
    "& \\hspace{20pt} \\text{Rescale all traces} \\hspace{2pt} e(s, a) = \\lambda e(s, a)  \\hspace{2pt} \\forall (s, a) \\\\\n",
    "& \\hspace{20pt} \\text{Choose} \\hspace{2pt} a_t \\sim \\pi(\\cdot \\vert s_t),  \\hspace{2pt} \\text{observe}  \\hspace{2pt} r_t  \\hspace{2pt} \\text{and}  \\hspace{2pt} s_{t+1} \\\\\n",
    "&  \\hspace{20pt} e(s_t, a_t) = e(s_t, a_t) + 1 \\\\\n",
    "&  \\hspace{20pt} \\text{Update} \\hspace{2pt} Q(s, a) = Q(s, a) + \\alpha * (r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)) e(s, a) \\hspace{2pt} \\forall (s, a)\\\\\n",
    "&  \\hspace{20pt} t \\gets t+1 \\\\\n",
    "& \\text{End while} \\\\\n",
    "& \\hspace{-13pt} \\text{End for}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a32875",
   "metadata": {},
   "source": [
    "### Imports and examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa023c6d",
   "metadata": {},
   "source": [
    "Please import the $\\texttt{TMaze}$ environment from `environment1.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977b5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the BinaryTreeMaze environment\n",
    "from environments.environment1 import TMaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c171b",
   "metadata": {},
   "source": [
    "The environment used in this exercise session is a T Maze. You can find a sketch of the environment in `environment1.py` or by running the cell below. Starting from the bottom of the Maze, the agent's goal is basically to:\n",
    "\n",
    "1. Learn quickly to arrive at the bifurcation of the T Maze.\n",
    "2. Once there, learn the direction (left or right) giving the highest reward.\n",
    "\n",
    "The possible actions that the agent can take from a generic state are \"u\" (up), \"d\" (down), \"l\" (left) or \"r\" (right). Infeasible moves, such as going down, left or right from the initial state, are forbidden. Reaching the goal state on the left gives a +1 reward, while the reward for reaching the right end-state of the bifurcation is +2. All other actions have a reward equal to 0.\n",
    "\n",
    "Because we are interested in different discretizations of the maze to test the advantage of eligibility traces, to initialize the environment you should, in general, type\n",
    "\n",
    "```\n",
    "env = TMaze(a, b)\n",
    "```\n",
    "\n",
    "where $a$ is the number of steps required from the bifurcation to the two rewarded states and $b$ is the number of steps the agent must move up to arrive at the bifurcation from the initial state. Thus, the state representation follows this convention:\n",
    "\n",
    "1. The initial state is the origin $(0, 0)$.\n",
    "2. The bifurcation state is labelled as $(0, b)$.\n",
    "3. The two rewarded states are in positions $(-a, b)$ (reward=1) and $(a, b)$ (reward=2) respectively.\n",
    "\n",
    "Additionally, the environment has the following methods:\n",
    "\n",
    "- `end`: Attribute of the class that becomes true when the environment is in one of the goal states.\n",
    "- `get_state()`: Returns the current state.\n",
    "- `reset()`: Reset the environment, i.e. set the state back to the origin $(0, 0)$ and set the accumulated reward to $0$.\n",
    "- `get_initial_state()`: Returns the starting point of every training episode. This statically returns $(0, 0)$.\n",
    "- `get_num_actions()`: Returns the number of maximum available actions. This statically returns `4` for this environment.\n",
    "- `get_num_states()`: Returns the number of possible states. This corresponds to $2a + b + 1$ where $a$ and $b$ are defined above.\n",
    "- `get_direct_path_len()`: Returns the length of the direct path from the starting state to the rewarded states, namely $a+b$.\n",
    "- `available()`: Returns a list of available actions from the current state, i.e. a subset of `[\"u\", \"d\", \"l\", \"r\"]`.\n",
    "- `do_action(action)`: Takes the action `action` in the current state. It returns a tuple `(state, reward)` corresponding to the new state after the action is taken and the reward obtained by the agent along the transition.\n",
    "- `reward()`: Returns the reward for getting to the current state. This is a reward of $1$ if the agent reaches the goal state $(-a, b)$, a reward of $2$ if the agent reaches the goal state $(a, b)$, a reward of $0$ otherwise.\n",
    "- `encode_action(action)`: Maps the strings of the possible actions `[\"u\", \"d\", \"l\", \"r\"]` to the integers `[0, 1, 2, 3]`.\n",
    "- `inverse_encoding(action)`: Inverse map of `encode_action(action)`.\n",
    "- `neighbours()`: Returns a list of the neighbours states of the current state of the environment.\n",
    "- `render(Q=None)`: Prints the current game state. If no set of Q-values is passed (run the cell below for an example), this functions simply plots the environment: the starting state is marked by a green circle, the two goal states are represented by a blue star and the current position of the agent by a red cross. If a set of Q-values is passde, it shows a heatmap of the Q-values passed in input (see examples below after your implementation of the algorithms).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac042d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the environment with default parameters\n",
    "env = TMaze(2, 5)\n",
    "\n",
    "# Render the maze and show the plot \n",
    "env.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Additionally to carrying out a comparison between TD algorithms with and without traces, you will also compare two different policies for action selection during training:\n",
    "\n",
    "1. **$\\mathbf{\\epsilon}$-greedy policy**: given a state $s$ and a set of $Q$-values $Q$, the $\\epsilon$-greedy policy chooses:\n",
    "\n",
    "- with probability $1-\\epsilon$ the action $a^{\\star} = \\mathrm{argmax}_{a}Q(s,a)$, where ties are broken randomly;\n",
    "- with probability $\\epsilon$ a uniformly chosen random action.\n",
    "\n",
    "2. $\\textbf{Softmax policy}$: given a state $s$ and a set of $Q$-values $Q$, it chooses the actions sampling from the probability distribution $\\pi(\\cdot\\vert s)$ defined as\n",
    "   $$\n",
    "   \\begin{equation}\n",
    "   \\pi(a\\vert s) = \\frac{\\exp(\\beta Q(s, a))}{\\sum_{a'}\\exp(\\beta Q(s, a'))},\n",
    "   \\end{equation}\n",
    "   $$\n",
    "   where $\\beta>0$ is the so-called scaling parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Implement utilities for action selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Implement two auxiliary functions in `algorithms_template.py`, called `epsilon_greedy` and `softmax_`, to perform action selection given a set of $Q$-values $Q$. _Hint: this function should return an available action given the current state of the environment_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from RL_algorithms.algorithms_template import epsilon_greedy, softmax_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4839ed",
   "metadata": {},
   "source": [
    "### Exercise 0: One step horizon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f8c3b",
   "metadata": {},
   "source": [
    "Start by considering a one step horizon reward scheme, i.e. the highest rewarded states are immediately reached after one step from the beginning. To do this, set $a = 1$ and $b = 0$ in the initialization of the environment.\n",
    "\n",
    "Implement the $\\text{Sarsa}$ algorithm in the `algorithms_template.py` file as you have seen it in the classroom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "063d5334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the environment with default parameters\n",
    "env = TMaze(1, 0)\n",
    "\n",
    "# Render the maze and show the plot \n",
    "env.render()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.algorithms_template import sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85022d13",
   "metadata": {},
   "source": [
    "Before running any further experiment, answer the following questions:\n",
    "\n",
    "1. Assume the actions \"left\" and \"right\" are chosen alternatively and all $Q$-values are initialized at zero. For which learning rate $\\alpha$ do you observe convergence of the $Q$-values in the minimum number of steps? Write down your calculations explicitly.\n",
    "2. For a general policy, find a lower bound on the number of steps needed for convergence of the $Q$-values as a function of the learning rate $\\alpha < 1$. In this case, by convergence we mean that both estimates of the $Q$-values should be at least $95 \\%$ of their true values (e.g. the $Q$-value corresponding to going left should be at least 0.95 and the $Q$-value for going right should be at least 1.9). _Hint: use the fact that every episode only lasts 1 move and, starting from the update rule of Sarsa for the one-step horizon, write a recurrence relation for the update of the two $Q$-values. Then think of the worst case scenario to conclude._\n",
    "3. Under an $\\epsilon$-greedy policy, which value of $\\epsilon$ will make training fail for sure?\n",
    "\n",
    "Conclude the analysis you have done in Questions 1-3 by stating a couple $(\\epsilon, \\alpha)$ for which training 1 episode or training 100, 1000, ... episodes will give the same results over and over.\n",
    "\n",
    "4. How does the use of a softmax policy change your answer to the question above? Which values of $\\beta$ give an almost exploitatory action selection during training?\n",
    "\n",
    "Verify your answers to the questions above numerically: test your implementation of Sarsa on the one step horizon environment and check convergence of the $Q$-values.\n",
    "\n",
    "Additionally, feel free to test the influence of the learning rate $\\alpha$ on the convergence of the $Q$-values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dae422",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TMaze(1, 0)\n",
    "\n",
    "###### GREEDY POLICY #########\n",
    "print(\"============ GREEDY POLICY ==============\")\n",
    "alpha = 0.5  # alpha = 0.5 means at least 6 episodes\n",
    "Q, stats = sarsa(env, num_episodes=5, epsilon_exploration=0, alpha=alpha) \n",
    "print(\"5 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=10, epsilon_exploration=0, alpha=alpha)\n",
    "print(\"10 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=15, epsilon_exploration=0, alpha=alpha)\n",
    "print(\"15 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=20, epsilon_exploration=0, alpha=alpha)\n",
    "print(\"20 episodes ---> \", Q[(0, 0)])\n",
    "\n",
    "###### 0.5 GREEDY POLICY #########\n",
    "print(\"============ 0.5 GREEDY POLICY ==========\")\n",
    "alpha = 0.5  # alpha = 0.5 means at least 6 episodes\n",
    "Q, stats = sarsa(env, num_episodes=5, epsilon_exploration=0.5, alpha=alpha) \n",
    "print(\"5 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=10, epsilon_exploration=0.5, alpha=alpha)\n",
    "print(\"10 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=15, epsilon_exploration=0.5, alpha=alpha)\n",
    "print(\"15 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=20, epsilon_exploration=0.5, alpha=alpha)\n",
    "print(\"20 episodes ---> \", Q[(0, 0)])\n",
    "\n",
    "###### EXPLORATORY SOFTMAX POLICY #########\n",
    "print(\"============ EXPLORATORY SOFTMAX POLICY ==============\")\n",
    "alpha = 0.5  # alpha = 0.5 means at least 6 episodes\n",
    "Q, stats = sarsa(env, num_episodes=5, action_policy='softmax_', epsilon_exploration=1, alpha=alpha) \n",
    "print(\"5 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=10, action_policy='softmax_', epsilon_exploration=1, alpha=alpha)\n",
    "print(\"10 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=15, action_policy='softmax_', epsilon_exploration=1, alpha=alpha)\n",
    "print(\"15 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=20, action_policy='softmax_', epsilon_exploration=1, alpha=alpha)\n",
    "print(\"20 episodes ---> \", Q[(0, 0)])\n",
    "\n",
    "###### EXPLOITATORY SOFTMAX POLICY #########\n",
    "print(\"============ EXPLOITATORY SOFTMAX POLICY ==============\")\n",
    "alpha = 0.5  # alpha = 0.5 means at least 6 episodes\n",
    "Q, stats = sarsa(env, num_episodes=5, action_policy='softmax_', epsilon_exploration=100, alpha=alpha) \n",
    "print(\"5 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=10, action_policy='softmax_', epsilon_exploration=100, alpha=alpha)\n",
    "print(\"10 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=15, action_policy='softmax_', epsilon_exploration=100, alpha=alpha)\n",
    "print(\"15 episodes ---> \", Q[(0, 0)])\n",
    "Q, stats = sarsa(env, num_episodes=20, action_policy='softmax_', epsilon_exploration=100, alpha=alpha)\n",
    "print(\"20 episodes ---> \", Q[(0, 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5a18c",
   "metadata": {},
   "source": [
    "### Exercise 1: Implementation of TD algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f42c2",
   "metadata": {},
   "source": [
    "In this exercise, you are asked to implement the Q-Learning($\\lambda$), Sarsa($\\lambda$) and $n$-step Sarsa algorithms in the cells below. To help you in the implementation, we guide you through the main steps of the implementation and ask you to fill in the missing parts accordingly to the hints in `algorithms_template.py`.\n",
    "\n",
    "In order to understand fully how the available code works, we recommend you to have a look at the T Maze environment implementation in `environment1.py`.\n",
    "\n",
    "$\\texttt{Remark 1}$: In the upcoming weeks, you will be asked to repeat numerical experiments on other environments. In order to reuse the code you will develop in the cells below, make sure your implementation is general enough and works for any other environment derived from the abstract environment in `abstract_environment.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146b5ba",
   "metadata": {},
   "source": [
    "#### Implementation of Q-Learning($\\lambda$), Sarsa($\\lambda$) and $n$-step Sarsa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec976a",
   "metadata": {},
   "source": [
    "Using the two auxiliary functions for action selection above, implement the $Q$-Learning($\\lambda$), Sarsa($\\lambda$) and $n$-step Sarsa algorithms. In the `algorithms_template.py` file you find some hints to guide you in the implementations.\n",
    "\n",
    "Before implementing the algorithms, answer the following question:\n",
    "\n",
    "1. For which value of $\\lambda$ do you recover the standard versions of $Q$-Learning and Sarsa algorithms?\n",
    "\n",
    "Make sure that your implementation recovers the standard algorithms for such a value of the trace decay parameter. For Sarsa, you may simply extend the code you delevoped above with proper changes (i.e. adding the possibility of a non-zero trace decay parameter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40081b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.algorithms_template import q_learning, sarsa, n_step_sarsa\n",
    "env = TMaze(2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88c180",
   "metadata": {},
   "source": [
    "#### Test your algorithms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29c931b7",
   "metadata": {},
   "source": [
    "Before moving on to further experiments, please test your implementation of the RL algorithms with the T-Maze environment.\n",
    "To do so, you can use the provided helper function `play` in `utils_template.py`, which, given a set of _Q_-values, simulates the moves of the agent in the maze using a greedy policy (based on the given _Q_-values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.utils_template import play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a154e",
   "metadata": {},
   "source": [
    "Now, run the algorithms\n",
    "\n",
    "1. $Q$-Learning($\\lambda$) for $\\lambda = 0$ and $\\lambda \\neq 0$,\n",
    "2. Sarsa($\\lambda$) for $\\lambda = 0$ and $\\lambda \\neq 0$,\n",
    "3. $n$-step Sarsa\n",
    "\n",
    "that you implemented above for a sufficient number of episodes and, using the function you wrote in the previous cell, check that the agent learns to reach the highest rewarded state.\n",
    "\n",
    "For the sake of convenience and in order to let you have a fair comparison of your results with your classmates, we recommend to set:\n",
    "\n",
    "- the discount factor to $\\gamma = 0.9$\n",
    "- the trace decay factor in _Sarsa($\\lambda$)_ and _Q-Learning($\\lambda$)_ to $\\lambda = 0.8$\n",
    "- the number of steps for $n$-step Sarsa to $n = 3$\n",
    "- the number of episodes played during learning to $n_{\\mathrm{episodes}} = 200$\n",
    "- the learning rate to $\\alpha=0.1$\n",
    "- the exploration parameter to $\\epsilon=0.5$\n",
    "\n",
    "For each of the $5$ algorithms you just tested, answer additionally the following questions:\n",
    "\n",
    "1. Do you think the $Q$-values have converged? Why? Which $Q$-values are the most difficult to be updated and why? _Hint: For visualization of the learned Q-values, recall that you can type `env.render(Q)` with the obtained set of $Q$-values._\n",
    "2. Has the policy converged?\n",
    "3. Just based on these heatmaps of the $Q$-values, do you observe any difference in the use of $n$-step TD methods/TD methods with eligibility traces with respect to the standard TD methods.\n",
    "\n",
    "Feel free to explore different hyperparameters' configurations for your own curiosity and to better understand their influence on the answers to the questions above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54134c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning(0)\n",
    "Q, stats = q_learning(env, gamma=0.9, num_episodes=200, epsilon_exploration=0.5, alpha=0.1)\n",
    "env.render(Q)\n",
    "play(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa(0)\n",
    "Q, stats = sarsa(env, gamma=0.9, num_episodes=200, epsilon_exploration=0.5, alpha=0.1)\n",
    "env.render(Q)\n",
    "play(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f644c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q-Learning(\\lambda)\n",
    "Q, stats = q_learning(env, gamma=0.9, num_episodes=200, epsilon_exploration=0.5, alpha=0.1, trace_decay=0.8)\n",
    "env.render(Q)\n",
    "play(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa(\\lambda)\n",
    "Q, stats = sarsa(env, gamma=0.9, num_episodes=200, epsilon_exploration=0.5, alpha=0.1, trace_decay=0.8)\n",
    "env.render(Q)\n",
    "play(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n-step Sarsa\n",
    "Q, stats = n_step_sarsa(env, gamma=0.9, num_episodes=200, epsilon_exploration=0.5, alpha=0.1, n=3)\n",
    "env.render(Q)\n",
    "play(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53add5",
   "metadata": {},
   "source": [
    "### Exercise 2: Exploration-Exploitation dilemma\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5db0d22d",
   "metadata": {},
   "source": [
    "You are now going to explore different exploration strategies during training to see how learning is affected. To do this, let us assume an $\\epsilon$-greedy policy for the action selection during training.\n",
    "\n",
    "Answer the following questions **before** running any experiment:\n",
    "\n",
    "1. Assume $\\epsilon = 0$, i.e. the agent always chooses the best action available according to the current estimates of the $Q$-values. What is the problem of this kind of approach? Which part of the state-space will remain unexplored? _Hint: all $Q$-values are initialized at zero_.\n",
    "2. What do you think can be the effect of the exploration parameter $\\epsilon$ on the efficiency of learning?\n",
    "\n",
    "Now, **verify experimentally** your results by using $Q$-Learning($\\lambda = 0$) as training algorithm. For a fair comparison with your classmates, set:\n",
    "\n",
    "- the discount factor to $\\gamma = 0.9$\n",
    "- the number of episodes played during learning to $n_{\\mathrm{episodes}} = 200$\n",
    "- the learning rate to $\\alpha=0.1$\n",
    "\n",
    "Test the values $\\epsilon = 0, 0.2, 0.5, 0.8, 1$ using the functions provided in `utils_template.py`. Are the numerical results you observe in line with what you expected? Which value of $\\epsilon$ is the best one in your opinion?\n",
    "\n",
    "Additionally, you can test for your own curiosity the convergence of the $Q$-values to their true value (using `env.render(Q)`) for different values of $\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c151e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.utils_template import *\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "num_avg = 30\n",
    "\n",
    "params = {\n",
    "    'gamma' : gamma,\n",
    "    'alpha' : alpha,\n",
    "    'action_policy': \"epsilon_greedy\"\n",
    "}\n",
    "\n",
    "Q_LEARNING_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\epsilon = 0$',\n",
    "    'params': {**params, 'epsilon_exploration': 0}\n",
    "}\n",
    "\n",
    "Q_LEARNING_20_PERCENT_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\epsilon = 0.2$',\n",
    "    'params': {**params, 'epsilon_exploration': 0.2}\n",
    "}\n",
    "\n",
    "Q_LEARNING_50_PERCENT_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\epsilon = 0.5$',\n",
    "    'params': {**params, 'epsilon_exploration': 0.5}\n",
    "}\n",
    "\n",
    "Q_LEARNING_80_PERCENT_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\epsilon = 0.8$',\n",
    "    'params': {**params, 'epsilon_exploration': 0.8}\n",
    "}\n",
    "\n",
    "Q_LEARNING_RANDOM = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\epsilon = 1$',\n",
    "    'params': {**params, 'epsilon_exploration': 1}\n",
    "}\n",
    "\n",
    "\n",
    "algorithms = [Q_LEARNING_GREEDY, Q_LEARNING_20_PERCENT_GREEDY, Q_LEARNING_50_PERCENT_GREEDY,\n",
    "              Q_LEARNING_80_PERCENT_GREEDY, Q_LEARNING_RANDOM]\n",
    "\n",
    "env = TMaze(2, 5)\n",
    "compare_episodes_lengths_and_rewards(env=env, algos=algorithms, num_avg=num_avg,\n",
    "                                     show_std=True, additional_params=[{\"num_episodes\": 200}] * len(algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72edb632",
   "metadata": {},
   "source": [
    "Assume now instead a softmax policy for action selection. Under this policy, each action has a non-zero probability to be selected. Moreover, exploration is controlled by the scaling parameter $\\beta$.\n",
    "\n",
    "Answer the following question **before** running any experiment:\n",
    "\n",
    "1. Which values of $\\beta$ should guarantee a better exploration during training? Which values guarantee, instead, an almost greedy policy?\n",
    "2. What do you think can be the effect of $\\beta$ on the performance? Why?\n",
    "\n",
    "Set $\\beta = 0.1, 1, 5, 10, 100$ and repeat the experiments you carried out before. Are the numerical results you observe in line with what you expected? Which value of $\\beta$ is the best in your opinion?\n",
    "\n",
    "Additionally, you can test for your own curiosity the convergence of the $Q$-values to their true value (using `env.render(Q)`) for different values of $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d24f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.utils_template import *\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "num_avg = 30\n",
    "\n",
    "params = {\n",
    "    'gamma' : gamma,\n",
    "    'alpha' : alpha,\n",
    "    'action_policy': 'softmax_'\n",
    "}\n",
    "\n",
    "Q_LEARNING_NON_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\beta = 0.1$',\n",
    "    'params': {**params, 'epsilon_exploration': 0.1}\n",
    "}\n",
    "\n",
    "Q_LEARNING_1_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\beta = 1$',\n",
    "    'params': {**params, 'epsilon_exploration': 1}\n",
    "}\n",
    "\n",
    "Q_LEARNING_5_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\beta = 5$',\n",
    "    'params': {**params, 'epsilon_exploration': 5}\n",
    "}\n",
    "\n",
    "Q_LEARNING_10_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\beta = 10$',\n",
    "    'params': {**params, 'epsilon_exploration': 10}\n",
    "}\n",
    "\n",
    "Q_LEARNING_100_GREEDY = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': r'$\\beta = 100$',\n",
    "    'params': {**params, 'epsilon_exploration': 100}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "algorithms = [Q_LEARNING_NON_GREEDY, Q_LEARNING_1_GREEDY, Q_LEARNING_5_GREEDY, Q_LEARNING_10_GREEDY, Q_LEARNING_100_GREEDY]\n",
    "\n",
    "env = TMaze(2, 5)\n",
    "compare_episodes_lengths_and_rewards(env=env, algos=algorithms, num_avg=num_avg,\n",
    "                                     show_std=True, additional_params=[{\"num_episodes\": 200}] * len(algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043af007",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparison of RL algorithms for different discretization schemes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55bbec09",
   "metadata": {},
   "source": [
    "As stated in the [introduction](introduction), the main goal of this computational exercise session is to understand how the coupling of TD algorithms with eligibility traces can provide benefits for the learning of an agent with respect to the case $\\lambda = 0$.\n",
    "\n",
    "To this aim, we consider a discretization of the T-Maze with 6, 11, 20 states and test the performance of different RL algorithms on these environments.\n",
    "\n",
    "Answer these questions **before** running any experiments:\n",
    "\n",
    "1. Consider the case in which the agent is trained with $\\lambda = 0$ on a T-Maze($a, b$). How many episodes are necessary to propagate back the reward information from the goal states to the origin? _Hint: all $Q$-values are initialized at zero._\n",
    "2. Does your answer change if $\\lambda \\neq 0$?\n",
    "\n",
    "Now, consider the following hyperparameter configuration:\n",
    "\n",
    "- the discount factor to $\\gamma = 0.9$\n",
    "- the trace decay factor in _Sarsa($\\lambda$)_ and _Q-Learning($\\lambda$)_ to $\\lambda = 0.8$\n",
    "- the number of episodes played during learning to $n_{\\mathrm{episodes}} = 50$\n",
    "- the learning rate to $\\alpha=0.1$\n",
    "- the exploration parameter $\\epsilon$ to the best value according to your experiments in Exercise 2.\n",
    "\n",
    "**Verify experimentally** your answers to the questions above. Again, to do so you may use the functions provided in `utils_template.py`. What are you observing? Answer the questions below.\n",
    "\n",
    "1. Is the performance of the $Q$-Learning and Sarsa independent of the discretization?\n",
    "2. What happens instead for $Q$-Learning($\\lambda$) and Sarsa($\\lambda$)? Can you claim undeniably that the performance is independent of the discretization scheme? _Hint: In the experiments above, $\\lambda$ is kept fixed for the sake of simplicity; but can we choose a more meaningful scheme?._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74221a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_algorithms.utils_template import *\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon_exploration = None   # CHANGE TO YOUR BEST VALUE\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "trace_decay = 0.8\n",
    "num_avg = 30\n",
    "\n",
    "params = {\n",
    "    'epsilon_exploration' : epsilon_exploration,\n",
    "    'gamma' : gamma,\n",
    "    'alpha' : alpha,\n",
    "}\n",
    "\n",
    "Q_LEARNING = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': 'Q-Learning(0)',\n",
    "    'params': params,\n",
    "}\n",
    "\n",
    "Q_LEARNING_LAMBDA = {\n",
    "    'algo_name': 'q_learning',\n",
    "    'name': 'Q-Learning($\\lambda$)',\n",
    "    'params': {**params, 'trace_decay': trace_decay}\n",
    "}\n",
    "\n",
    "SARSA = {\n",
    "    'algo_name': 'sarsa',\n",
    "    'name': 'Sarsa(0)',\n",
    "    'params': params\n",
    "}\n",
    "\n",
    "SARSA_LAMBDA = {\n",
    "    'algo_name': 'sarsa',\n",
    "    'name': 'Sarsa($\\lambda$)',\n",
    "    'params': {**params, 'trace_decay': trace_decay}\n",
    "}\n",
    "\n",
    "THREE_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '3-step Sarsa',\n",
    "    'params': {**params, 'n': 3}\n",
    "}\n",
    "\n",
    "algorithms = [Q_LEARNING, Q_LEARNING_LAMBDA, SARSA, SARSA_LAMBDA, THREE_STEP_SARSA]\n",
    "\n",
    "couples = ((1, 3), (2, 6), (3, 13))\n",
    "for j, couple in enumerate(couples):\n",
    "    env = TMaze(*couple)\n",
    "    compare_episodes_lengths_and_rewards(env=env, algos=algorithms, num_avg=num_avg,\n",
    "                                         show_std=True, additional_params=[{\"num_episodes\": 50}] * len(algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f362949",
   "metadata": {},
   "source": [
    "### Exercise 4: Rescaling of the trace decay and step parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cab614",
   "metadata": {},
   "source": [
    "Answer the following questions **before** running any experiment:\n",
    "\n",
    "Consider a T-Maze environment with $s$ states separating the starting state from each of the goal states, i.e $s = a+b$.\n",
    "\n",
    "1. How should you rescale the number of steps used for the $n$-step Sarsa algorithm to have a similar performance if $s$ is doubled, i.e. $s \\gets 2s$?\n",
    "\n",
    "**Verify experimentally** your results by looking again at the running average of the reward and at the episode lengths during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa879a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epsilon_exploration = None   # CHANGE TO YOUR BEST VALUE\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "num_avg = 30\n",
    "\n",
    "params = {\n",
    "    'epsilon_exploration' : epsilon_exploration,\n",
    "    'gamma' : gamma,\n",
    "    'alpha' : alpha,\n",
    "    'num_episodes': 50\n",
    "}\n",
    "\n",
    "ONE_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '1-step Sarsa',\n",
    "    'params': {**params, 'n': 1}\n",
    "}\n",
    "\n",
    "TWO_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '2-step Sarsa',\n",
    "    'params': {**params, 'n': 2}\n",
    "}\n",
    "\n",
    "FOUR_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '4-step Sarsa',\n",
    "    'params': {**params, 'n': 4}\n",
    "}\n",
    "\n",
    "EIGHT_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '8-step Sarsa',\n",
    "    'params': {**params, 'n': 8}\n",
    "}\n",
    "\n",
    "SIXTEEN_STEP_SARSA = {\n",
    "    'algo_name': 'n_step_sarsa',\n",
    "    'name': '16-step Sarsa',\n",
    "    'params': {**params, 'n': 16}\n",
    "}\n",
    "\n",
    "algorithms = {'0': [ONE_STEP_SARSA, TWO_STEP_SARSA, FOUR_STEP_SARSA],\n",
    "              '1': [TWO_STEP_SARSA, FOUR_STEP_SARSA, EIGHT_STEP_SARSA],\n",
    "              '2': [FOUR_STEP_SARSA, EIGHT_STEP_SARSA, SIXTEEN_STEP_SARSA]}\n",
    "\n",
    "couples = ((2, 5), (4, 10), (8, 20))\n",
    "\n",
    "for j, couple in enumerate(couples):\n",
    "    env = TMaze(*couple)\n",
    "    compare_episodes_lengths_and_rewards(env=env, algos=algorithms[str(j)], num_avg=num_avg,\n",
    "                                         show_std=True, additional_params=[{\"num_episodes\": 50}] * len(algorithms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbd14a6e8332d760f6bee624cd963a1125e31bcf675ce035ea70247f809c8780"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
