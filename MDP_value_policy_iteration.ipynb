{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = np.zeros((3,2,3))  # states, actions, states\n",
    "probas[0, 0, 1] = 0.3\n",
    "probas[0, 0, 2] = 0.7\n",
    "probas[0, 1, 2] = 1\n",
    "probas[1, 0, 0] = 1\n",
    "probas[2, 0, 1] = 1\n",
    "probas[2, 1, 2] = 1\n",
    "\n",
    "rewards = np.zeros((3,2))   # states, actions\n",
    "rewards[0, 0] = 0\n",
    "rewards[0, 1] = -1\n",
    "rewards[1, 0] = 2\n",
    "rewards[2, 0] = 2\n",
    "rewards[2, 1] = -3\n",
    "\n",
    "gamma = 0.9\n",
    "threshold = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, R, gamma, theta):\n",
    "    V = np.zeros(3)\n",
    "    Pol = np.zeros(3, dtype=int)   # only to track policies\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in [0, 1, 2]:\n",
    "            S = np.unique(np.argwhere(P[s,:,:] != 0)[:,1])\n",
    "            v = V[s]\n",
    "            V[s] = np.max([np.sum([P[s, a, s_]*(R[s, a] + gamma*V[s_]) for s_ in S]) for a in [0,1]])\n",
    "            Pol[s] = np.argmax([np.sum([P[s, a, s_]*(R[s, a] + gamma*V[s_]) for s_ in S]) for a in [0,1]])\n",
    "            delta = max(delta, np.abs(v-V[s]))\n",
    "        if delta < theta:\n",
    "            return V, Pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11.86962133, 12.6826592 , 13.41439328]), array([0, 0, 0]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_values, val_actions = value_iteration(probas, rewards, gamma, threshold)\n",
    "val_values, val_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, R, gamma, theta):\n",
    "    # 1\n",
    "    V = np.zeros(3)\n",
    "    Pol = np.ones(3, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        # 2\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in [0, 1, 2]:\n",
    "                S = np.unique(np.argwhere(P[s,:,:] != 0)[:,1])\n",
    "                v = V[s]\n",
    "                V[s] = np.sum([P[s, Pol[s], s_]*(R[s, Pol[s]] + gamma*V[s_]) for s_ in S])\n",
    "                Pol[s] = np.argmax([np.sum([P[s, a, s_]*(R[s, a] + gamma*V[s_]) for s_ in S]) for a in [0,1]])\n",
    "                delta = max(delta, np.abs(v-V[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        # 3\n",
    "        policy_stable = True\n",
    "        for s in [0, 1, 2]:\n",
    "            old_action = Pol[s]\n",
    "            Pol[s] = np.argmax([np.sum([P[s, a, s_]*(R[s, a] + gamma*V[s_]) for s_ in S]) for a in [0,1]])\n",
    "            if old_action != Pol[s]:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            return V, Pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([11.86590806, 12.67931725, 13.41138553]), array([0, 0, 0]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_values, pol_actions = policy_iteration(probas, rewards, gamma, threshold)\n",
    "pol_values, pol_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Value Iteration --------------\n",
      "Optimum Actions:\n",
      "S1: 1\n",
      "S2: 1\n",
      "S3: 1\n",
      "Max Values:\n",
      "S1: 11.87\n",
      "S2: 12.683\n",
      "S3: 13.414\n",
      "-------------- Policy Iteration --------------\n",
      "Optimum Actions:\n",
      "S1: 1\n",
      "S2: 1\n",
      "S3: 1\n",
      "Max Values:\n",
      "S1: 11.866\n",
      "S2: 12.679\n",
      "S3: 13.411\n"
     ]
    }
   ],
   "source": [
    "print('-------------- Value Iteration --------------')\n",
    "print('Optimum Actions:')\n",
    "[print(f'S{i+1}:',a+1) for i, a in enumerate(val_actions)];\n",
    "print('Max Values:')\n",
    "[print(f'S{i+1}:',round(v,3)) for i, v in enumerate(val_values)];\n",
    "print('-------------- Policy Iteration --------------')\n",
    "print('Optimum Actions:')\n",
    "[print(f'S{i+1}:',a+1) for i, a in enumerate(pol_actions)];\n",
    "print('Max Values:')\n",
    "[print(f'S{i+1}:',round(v,3)) for i, v in enumerate(pol_values)];\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
